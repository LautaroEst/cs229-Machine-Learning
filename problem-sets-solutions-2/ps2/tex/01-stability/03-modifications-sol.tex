\begin{answer}
\begin{enumerate}
    \item No. $L(\theta)$ can still be arbitrarily large.
  \item Yes. When the learning rate is sufficiently small, the update to $\theta$ will be small, and it will be judge to be converged by the algorithm.
  \item No. The dataset will still be linearly seperable.
  \item Yes. In this case, scaling $\theta$ will not make the objective arbitrarily large.
  \item Yes. This will very likely make the dataset not linearly seperable.
  \item Yes. This will make the dataset not linearly separable, and easier to approach the global minimum.
\end{enumerate}

\end{answer}
