{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolución del Assignment 1\n",
    "\n",
    "La idea sería ir haciendo un apunte con las cosas que voy aprendiendo de los assignments.\n",
    "\n",
    "---\n",
    "\n",
    "## Supervised Learning\n",
    "\n",
    "Lo primero que vemos son algoritmos de aprendizaje supervisado. En estos casos siempre tengo un conjunto de datos $train\\;set = \\{ (x_1,y_1), \\ldots, (x_N, y_N) \\}$, el cual generalmente voy a poder definir en forma matricial como\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "X_{train} &= \n",
    "\\begin{bmatrix} \n",
    "- x_1 - \\\\\n",
    "\\vdots \\\\\n",
    "- x_N - \\\\\n",
    "\\end{bmatrix} \\\\[1em]\n",
    "y_{train} &= \n",
    "\\begin{bmatrix} \n",
    "y_1 \\\\\n",
    "\\vdots \\\\\n",
    "y_N \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "de manera que en un programa resulte\n",
    "\n",
    "```Python\n",
    "\n",
    "# Levanto los datos\n",
    "X_train, y_train = # ...\n",
    "\n",
    "# La primera dimensión de X_train y de y_train \n",
    "# es N (o sea, la cantidad de datos de entrenamiento)\n",
    "(X_train.shape[0] == N) # True\n",
    "(y_train.shape[0] == N) # True\n",
    "\n",
    "```\n",
    "\n",
    "El objetivo final va a ser obtener una función, a partir de estos datos, que resuelva una determinada tarea. Por cuestiones históricas esta función se la llama \"hipótesis\" o \"modelo\" (no sé muy bien si se deberían usar estas palabras, pero bueno).\n",
    "\n",
    "### Algoritmo 1: Cuadrados Mínimos\n",
    "\n",
    "**Deducción de las ecuaciones normales.** Definimos nuestro modelo como $h_{\\theta} (x) = \\theta^T x$ y queremos encontrar los valores de $\\theta$ para los cuales el error cuadrático medio entre los predichos por $h_{\\theta}$ y las etiquetas es el mínimo. De esta forma, definimos el costo\n",
    "\n",
    "$$\n",
    "J\\left(\\theta \\right) = \\frac{1}{2} \\sum_{i=1}^N (\\theta^T x_i - y_i)^2 = \\frac{1}{2} \\left\\| X_{train} \\theta - y_{train} \\right\\|^2 \n",
    "$$\n",
    "\n",
    "calculamos el gradiente con respecto a $\\theta$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla J\\left(\\theta \\right) &= \\frac{1}{2} \\nabla \\left\\| X_{train} \\theta - y_{train} \\right\\|^2 \\\\[.5em]\n",
    "&= \\frac{1}{2} \\nabla \\left(X_{train} \\theta - y_{train} \\right)^T \\left(X_{train} \\theta - y_{train} \\right) \\\\[.5em]\n",
    "&= \\frac{1}{2} \\nabla \\left( \\|X_{train} \\theta\\|^2 - 2 y_{train}^T X_{train} \\theta + \\|y_{train}\\|^2 \\right) \\\\[.5em]\n",
    "&= \\frac{1}{2} \\left( 2 \\theta^T X_{train}^T X_{train} - 2 y_{train}^T X_{train} \\theta \\right)\\\\[.5em] \n",
    "&= \\theta^T X_{train}^T X_{train} - y_{train}^T X_{train} \\theta\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "e igualamos a cero:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla J\\left(\\theta \\right) &= 0 \\\\[.5em]\n",
    "\\theta^T X_{train}^T X_{train} - y_{train}^T X_{train} \\theta &= 0 \\\\[.5em]\n",
    "X_{train}^T X_{train} \\theta &= X_{train}^T y_{train}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Deducción del algoritmo LMS**. El algoritmo es medio raro porque no estoy usando todas las muestras al mismo tiempo, y por lo tanto tengo una función de costo distinta en cada iteración.\n",
    "\n",
    "**Interpretación probabilística.** Acá empieza la posta. La posta es que lo anterior se puede ver como un caso particular de la interpretación probabilística que se explica a continuación.\n",
    "\n",
    "Si el objetivo es encontrar una función $h_{\\theta}(x)$ que realice predicciones sobre nuevas muestras, entonces vamos a decir que la situación con la que contamos, en realidad, es la siguiente:\n",
    "\n",
    "1. La función $h_\\theta$ va a recibir realizaciones de un vector aleatorio $\\mathbf{x}$ y va a devolver una variable aleatoria $\\mathbf{y}$.\n",
    "2. El conjunto de datos de entrenamiento, compuesto por $X_{train}$ e $y_{train}$, son muestras de las variables aleatorias $\\mathbf{x}$ e $\\mathbf{y}$, respectivamente.\n",
    "3. Proponiendo una forma paramétrica de la distribución de probabilidad $p(y|x)$, puedo obtener la función $h_\\theta$ como $$ h_\\theta(x) = \\mathbb{E}\\left[ y|x \\right] $$. Es decir que si estimo $p(y|x)$, la función $h_\\theta(x)$ va a ser un estimador de la media condicional. \n",
    "\n",
    "Esto es un modelo discriminativo, y se puede generalizar todavía más. **Lo que me está diciendo es que identifique mis variables de entrada y de salida, estime la probabilidad condicional y me haga predicciones con el valor estimado de la media de esa ddp.**\n",
    "\n",
    "Hay muchas variantes sobre este algoritmo, las cuales incluyen muchas modificaciones informales de las funciones de costo (con pesos y/o términos de regularización), formas aproximadas de proponer las distribuciones de probabilidad (Naive Bayes) o cambios en la forma de la función de activación (que dejan de considerar la salida como una densidad de probabilidad)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
