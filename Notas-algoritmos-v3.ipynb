{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\DeclareMathOperator*{\\argmin}{argmin}\n",
    "\\DeclareMathOperator*{\\argmax}{argmax}\n",
    "\\DeclareMathOperator*{\\min}{min}\n",
    "\\DeclareMathOperator*{\\max}{max}\n",
    "\\newcommand{\\trace}{{\\mathrm{tra}}}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\newcommand{\\Q}{\\mathbb{Q}}\n",
    "\\newcommand{\\N}{\\mathbb{N}}\n",
    "\\newcommand{\\xtrain}{X_{train}}\n",
    "\\newcommand{\\pygivenx}{P(y|x)}\n",
    "\\newcommand{\\pxgiveny}{P(x|y)}\n",
    "\\newcommand{\\normal}[2]{\\mathcal{N}\\left(#1,#2\\right)}\n",
    "\\newcommand{\\normalmusigma}{\\mathcal{N}\\left(\\mu,\\sigma^2\\right)}\n",
    "$$\n",
    "\n",
    "\n",
    "# Notas del curso cs229 - Machine Learning (Versión 2)\n",
    "\n",
    "---\n",
    "\n",
    "El objetivo de estas notas en particular sería explicar e implementar los algoritmos del curso cs229. Estos serían, en primer lugar, los de las notas 1, 2 y 3 (aprendizaje supervisado), que creo que abarcan todos estos:\n",
    "\n",
    "* Regresión lineal\n",
    "* Regresión logística\n",
    "* GDA\n",
    "* Naive Bayes\n",
    "* SVM \n",
    "* Algoritmos con Kernels (ver)\n",
    "* algo más? \n",
    "\n",
    "Después habría que implementar los algoritmos de aprendizaje no supervisado de las notas 7a, 7b y 8 que son modelos de mezclas de gaussianas. \n",
    "\n",
    "Después empieza la parte de:\n",
    "\n",
    "* Factor analisys\n",
    "* PCA\n",
    "* ICA\n",
    "* algo más?\n",
    "\n",
    "que también son algoritmos que habría que tratar de implementar. Esto está en las notas 9, 10 y 11.  \n",
    "\n",
    "Y por último está la parte de Reinforcement Learning y todo eso que se puede relacionar con control y modelización de sistemas físicos. Esto está en las notas 12.\n",
    "\n",
    "## Modelos\n",
    "\n",
    "El objetivo de Machine Learning es hacer modelos a partir de un conjunto de datos. En este contexto, un \"modelo\" es una función que hace una tarea muy compleja y la hace bien. Por ejemplo, una función implementada a partir de una red neuronal convolucional que distingue imágenes de perros y de gatos es un modelo. Una regresión por cuadrados mínimos es un modelo. \n",
    "\n",
    "Ahora, cuanto más difícil sea la tarea, más difícil va a ser encontrar la función, con lo cual hay que pensar una estrategia. Esa estrategia se llama *Learning*, y es por eso que la idea de *Machine Learning* es diseñar un algoritmo que *aprenda* la función que está buscando. Es decir, sin ser muy rigurosos en qué significa \"aprender\" en este contexto, los algoritmos de Machine Learning encuentran funciones que realizan tareas muy complejas y lo hacen a partir de un conjunto de datos, del cual aprenden a resolver el problema de la mejor forma posible (según algún criterio).\n",
    "\n",
    "Entonces, un algoritmo de ML tiene los siguientes ingredientes:\n",
    "\n",
    "1. Una función $f(\\cdot)$ que se representa un \"modelo\".\n",
    "\n",
    "2. Un conjunto de datos $\\xtrain$ para encontrar $f(\\cdot)$\n",
    "\n",
    "3. Un criterio de *accuracy* que mide cuán bien se aprendió la función $f(\\cdot)$. \n",
    "\n",
    "4. Un algoritmo de aprendizaje propiamente dicho, que en general consiste de una función *loss* que contiene el criterio con el cual yo digo \"la función encontrada se ajusta a los datos\" y un criterio de optimización a partir de esa función.\n",
    "\n",
    "Para comenzar, vamos a estudiar los modelos de aprendizaje supervisado.\n",
    "\n",
    "## Modelos de aprendizaje supervisado\n",
    "\n",
    "En lo que sigue se van a plantear algoritmos que utilicen un conjunto de datos $\\xtrain = \\{ (x_1,y_1), \\ldots, (x_N,y_N) \\}$ para aprender la función. Este tipo de aprendizaje se llama \"supervisado\" porque se tiene las etiquetas. \n",
    "\n",
    "En estos modelos vamos a suponer que el conjunto $\\xtrain$ son realizaciones de un vector aleatorio $(\\mathbf{x},\\mathbf{y})$, de distribución conjunta $P(x,y)$, con lo cual toda la información que nos preveen los datos está contemplada en esa distribución. El objetivo final, entonces, sería estimar de algún modo esa función. Esto se puede hacer de varias formas.\n",
    "\n",
    "### 1. Modelos discriminativos.\n",
    "\n",
    "En este tipo de modelos vamos a buscar una función $f(\\cdot)$ que tome vectores como los $x_1,\\ldots,x_N$ y devuelva vectores como los $y_1,\\ldots,y_N$, de manera que el conjunto de datos quede representado de alguna manera por esa función. \n",
    "\n",
    "Para hacer eso, vamos a suponer que el conjunto $\\xtrain$ son realizaciones de un vector aleatorio $(\\mathbf{x},\\mathbf{y})$, de distribución conjunta $P(x,y) = \\pygivenx P(x)$, y que la función $f(\\cdot)$ que se está buscando es la que maximiza el *likelihood* del conjunto de datos $\\xtrain$, **asumiendo que se puede encontrar proponiendo una forma para $\\pygivenx$**. \n",
    "\n",
    "Es decir, los modelos discriminativos tratan de encontrar una función $f(\\cdot)$ que prediga nuevos resultados modelando la distribución $\\pygivenx$.\n",
    "\n",
    "\n",
    "**Ejemplo 1. Regresión por cuadrados mínimos.** Explicar regresión lineal...\n",
    "\n",
    "**Ejemplo 2. Regresión logística.** Explicar regresión logística...\n",
    "\n",
    "En general, el algoritmo *Generalized Linear Models* propone lo siguiente:\n",
    "\n",
    "1. Modelar la distribución $\\pygivenx$ como una distribución de la familia de exponenciales de parámetro $\\theta^T x$, donde lo que se va a estimar es el valor de $\\theta$.\n",
    "\n",
    "2. Estimar el valor de $\\theta$ a partir de los datos con *Maximum Likelihood*\n",
    "\n",
    "3. Obtener la función $f(\\cdot)$ como $f(x) = \\mathbb{E}\\left[ y|x \\right]$ una vez estimada $\\pygivenx$.\n",
    "\n",
    "\n",
    "\n",
    "### 2. Modelos generativos.\n",
    "\n",
    "Hay otra forma de hacer un modelo sin decir explícitamente \"la función de probabilidad $\\pygivenx$ tiene tal forma\". En lugar de eso, puedo suponer conocida las formas de $\\pxgiveny$ y de $P(y)$ para cada uno de los posibles valores de $y$. Esta elección va a depender del problema a resolver, puesto que a veces es más fácil una y a veces, la otra.\n",
    "\n",
    "En este caso se está haciendo un modelo por cada valor posible de $y$, y por eso se llaman \"generativos\". Porque se supone que las muestras son generadas por funciones distintas en cada caso.\n",
    "\n",
    "**Ejemplo 1. LDA.** Explicar LDA...\n",
    "\n",
    "**Ejemplo 2. Naive Bayes.** Explicar Naive Bayes con Laplace smoothing y con la generalización multinominal.\n",
    "\n",
    "\n",
    "De vuelta, hay que explicar lo de la familia de exponenciales, pero para generativos.(Ver Notas 2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
